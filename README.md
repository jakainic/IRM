# IRM

The aim of this project is to build (and train) the model described in the paper [Invariant Risk Minimization](https://arxiv.org/abs/1907.02893v1) by Arjovsky, et al. Specifically, I am aiming to reproduce the results from the Colored MNIST experiments described in that paper. 

I initially aimed to do hyperparameter tuning myself, but struggled due to limitations of my hardware. Ultimately, I used the hyperparameters as optimized by the authors, which can be found on [GitHub](https://github.com/facebookresearch/InvariantRiskMinimization). However, I experimented with both minibatch training as well as full batch training, whereas the authors' code is only for full batch training. So, I chose my own hyperparameters for choosing batch size, number of epochs, etc. 

However, I have so far not been able to reproduce the authors' results. My test accuracy hovers in the 30-35% range. In and of itself, this might suggest that the optimization is behaving like ERM, as both test and train accuracies reflect my test accuracy on my ERM baselines. Initially, I tried optimizing without annealing, thereby applying an amplifying weight on the gradient norm penalty at every step, rather than just after the annealing point. However, this did not improve the issue. Moreover, the issue seems to persist throughout the training process (when monitoring to check test accuracy every 50 epochs in the fullbatch case), so from this perspective it seems like the model isn't really learning the invariance constraint. That all being said, my ERM baselines are not really where they should be. Due to the way in which the testing environment is constructed, one would expect ERM to have about 10% accuracy on the test set, while mine are much higher. Thus, I suspect the issue is in fact not related to the penalty term (or at least, not exclusively that), as any problem there would not affect my ERM results. It is not impossible that there is a code issue, rather than a conceptual one. I will be doing further monitoring and experimenting to determine what is happening.

In the future, I would like to experiment with more environments of varying sizes. I would also like to consider extensions of these ideas to text data, particularly but not exclusively with fairness applications.
